{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "29ekoHpBCoKe",
    "outputId": "9d0e34d4-1b81-4e50-d7e3-353d0cdfee63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chess\n",
      "  Downloading chess-1.11.1.tar.gz (156 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: chess\n",
      "  Building wheel for chess (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for chess: filename=chess-1.11.1-py3-none-any.whl size=148497 sha256=d9a03b629e9b9a68d382e4e43ff321cb49286fe6c5153c9b6ebfb610d341ae3e\n",
      "  Stored in directory: /root/.cache/pip/wheels/2e/2d/23/1bfc95db984ed3ecbf6764167dc7526d0ab521cf9a9852544e\n",
      "Successfully built chess\n",
      "Installing collected packages: chess\n",
      "Successfully installed chess-1.11.1\n"
     ]
    }
   ],
   "source": [
    "!pip install chess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0G314LoRgNUn"
   },
   "source": [
    "# Part 1: Load Data\n",
    "\n",
    "Our raw data contains the details of a singular chess game. Our focus is on the move sequence (which we would use to construct game states) and player information (ELO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7pDGf1u13uHs",
    "outputId": "1cd76ef0-b3f9-402d-de42-7a83ff881af6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file loaded successfully with 20058 rows.\n",
      "         id  rated    created_at  last_move_at  turns victory_status winner  \\\n",
      "0  TZJHLljE  False  1.500000e+12  1.500000e+12     13      outoftime  white   \n",
      "1  l1NXvwaE   True  1.500000e+12  1.500000e+12     16         resign  black   \n",
      "2  mIICvQHh   True  1.500000e+12  1.500000e+12     61           mate  white   \n",
      "3  kWKvrqYL   True  1.500000e+12  1.500000e+12     61           mate  white   \n",
      "4  9tXo1AUZ   True  1.500000e+12  1.500000e+12     95           mate  white   \n",
      "\n",
      "  increment_code       white_id  white_rating      black_id  black_rating  \\\n",
      "0           15+2       bourgris          1500          a-00          1191   \n",
      "1           5+10           a-00          1322     skinnerua          1261   \n",
      "2           5+10         ischia          1496          a-00          1500   \n",
      "3           20+0  daniamurashov          1439  adivanov2009          1454   \n",
      "4           30+3      nik221107          1523  adivanov2009          1469   \n",
      "\n",
      "                                               moves opening_eco  \\\n",
      "0  d4 d5 c4 c6 cxd5 e6 dxe6 fxe6 Nf3 Bb4+ Nc3 Ba5...         D10   \n",
      "1  d4 Nc6 e4 e5 f4 f6 dxe5 fxe5 fxe5 Nxe5 Qd4 Nc6...         B00   \n",
      "2  e4 e5 d3 d6 Be3 c6 Be2 b5 Nd2 a5 a4 c5 axb5 Nc...         C20   \n",
      "3  d4 d5 Nf3 Bf5 Nc3 Nf6 Bf4 Ng4 e3 Nc6 Be2 Qd7 O...         D02   \n",
      "4  e4 e5 Nf3 d6 d4 Nc6 d5 Nb4 a3 Na6 Nc3 Be7 b4 N...         C41   \n",
      "\n",
      "                             opening_name  opening_ply  \n",
      "0        Slav Defense: Exchange Variation            5  \n",
      "1  Nimzowitsch Defense: Kennedy Variation            4  \n",
      "2   King's Pawn Game: Leonardis Variation            3  \n",
      "3  Queen's Pawn Game: Zukertort Variation            3  \n",
      "4                        Philidor Defense            5  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import chess\n",
    "\n",
    "def load_chess_data(filepath, nrows=None):\n",
    "    \"\"\"\n",
    "    Load chess game data from a CSV file into a Pandas DataFrame. By default, the entire file is loaded.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path to the CSV file.\n",
    "        nrows (int, optional): The number of rows to load. If None, the entire file is loaded.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The loaded DataFrame containing the chess data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the CSV file, either all rows or the first nrows rows\n",
    "        df = pd.read_csv(filepath, nrows=nrows)\n",
    "\n",
    "        # Ensure required columns are present\n",
    "        required_columns = ['winner', 'white_rating', 'black_rating', 'moves',\n",
    "                            'opening_eco', 'opening_name', 'opening_ply']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns in the CSV file: {missing_columns}\")\n",
    "\n",
    "        print(f\"CSV file loaded successfully with {len(df)} rows.\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the entire CSV file or specify the number of rows\n",
    "file_path = \"games.csv\"\n",
    "chess_df = load_chess_data(file_path)\n",
    "\n",
    "if chess_df is not None:\n",
    "    print(chess_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9XT-IRmge95"
   },
   "source": [
    "# Part 2: Game State Construction and Data Engineering\n",
    "\n",
    "We want to make a prediction of a winner (white win percentage) based on a particular board state. To extract the board states, we extract every single board state from each game, using the sequences of moves.\n",
    "\n",
    "We then calculate certain parameters from the game state and save all individual game states. The details are as follows. All differential values are (WHITE - BLACK).\n",
    "\n",
    "### Piece Differential\n",
    "Each remaining piece gets assigned a standard weight:\n",
    "- PAWN: 1\n",
    "- KNIGHT: 3\n",
    "- BISHOP: 3\n",
    "- ROOK: 5\n",
    "- QUEEN: 9\n",
    "- KING: 0 (as all states must have a king)\n",
    "The differntial is calculated by white's piece value minus that of black.\n",
    "\n",
    "### ELO Differential\n",
    "Difference of ELO rating.\n",
    "\n",
    "### Mobility\n",
    "Difference in number of legal moves. \n",
    "\n",
    "### King Safety\n",
    "Points are added for certain pawn sheild structures around the king, and are subtracted if less of these pawns exist. Return the difference between players.\n",
    "\n",
    "### Game Phase\n",
    "Return the game phase (opening, midgame, endgame) based on move number and remaining pieces. \n",
    "\n",
    "### Pawn Structure\n",
    "Two types of pawn structures are evaluated:\n",
    "- Isolated pawns\n",
    "- Doubled pawns\n",
    "Differential is calculated between white and black, and returned. \n",
    "\n",
    "### Control of Key Squares\n",
    "Bonus points for controlling crucial middle section squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rW6NSYkhF2NQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import chess\n",
    "import chess.pgn\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_piece_differential(board):\n",
    "    piece_values = {\n",
    "        chess.PAWN: 1,\n",
    "        chess.KNIGHT: 3,\n",
    "        chess.BISHOP: 3,\n",
    "        chess.ROOK: 5,\n",
    "        chess.QUEEN: 9,\n",
    "        chess.KING: 0  # Assign 0 value to the King\n",
    "    }\n",
    "    white_value = sum(\n",
    "        piece_values.get(piece.piece_type, 0)\n",
    "        for piece in board.piece_map().values()\n",
    "        if piece.color == chess.WHITE\n",
    "    )\n",
    "    black_value = sum(\n",
    "        piece_values.get(piece.piece_type, 0)\n",
    "        for piece in board.piece_map().values()\n",
    "        if piece.color == chess.BLACK\n",
    "    )\n",
    "    return white_value - black_value\n",
    "\n",
    "# CALCULATED BUT UNUSED\n",
    "def calculate_elo_diff(df):\n",
    "    \"\"\"\n",
    "    Calculate the Elo difference (white_rating - black_rating).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The chess dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with the 'elo_diff' column.\n",
    "    \"\"\"\n",
    "    df['elo_diff'] = df['white_rating'] - df['black_rating']\n",
    "    return df\n",
    "\n",
    "def calculate_mobility(board):\n",
    "    \"\"\"\n",
    "    Calculate the mobility difference (white_moves - black_moves).\n",
    "\n",
    "    Args:\n",
    "        board (chess.Board): The current game state.\n",
    "\n",
    "    Returns:\n",
    "        int: The difference in the number of legal moves.\n",
    "    \"\"\"\n",
    "    original_turn = board.turn\n",
    "    board.turn = chess.WHITE\n",
    "    white_moves = len(list(board.legal_moves))\n",
    "    board.turn = chess.BLACK\n",
    "    black_moves = len(list(board.legal_moves))\n",
    "    board.turn = original_turn  # Restore the original turn\n",
    "    return white_moves - black_moves\n",
    "\n",
    "def calculate_king_safety(board):\n",
    "    \"\"\"\n",
    "    Calculate the king safety differential.\n",
    "\n",
    "    Args:\n",
    "        board (chess.Board): The current game state.\n",
    "\n",
    "    Returns:\n",
    "        float: The difference in king safety scores (white - black).\n",
    "    \"\"\"\n",
    "    def king_safety_for_color(color):\n",
    "        king_square = board.king(color)\n",
    "        safety_score = 0\n",
    "\n",
    "        # Check if the king is castled\n",
    "        if color == chess.WHITE:\n",
    "            if king_square == chess.E1:\n",
    "                safety_score -= 1  # Less safe if not castled\n",
    "            elif king_square in [chess.G1, chess.C1]:\n",
    "                safety_score += 1  # More safe if castled\n",
    "        else:\n",
    "            if king_square == chess.E8:\n",
    "                safety_score -= 1\n",
    "            elif king_square in [chess.G8, chess.C8]:\n",
    "                safety_score += 1\n",
    "\n",
    "        # Check for pawn shield around the king\n",
    "        pawn_shield_squares = []\n",
    "        if color == chess.WHITE:\n",
    "            if king_square == chess.G1:\n",
    "                pawn_shield_squares = [chess.F2, chess.G2, chess.H2]\n",
    "            elif king_square == chess.C1:\n",
    "                pawn_shield_squares = [chess.A2, chess.B2, chess.C2]\n",
    "            else:\n",
    "                pawn_shield_squares = [chess.D2, chess.E2, chess.F2]\n",
    "        else:\n",
    "            if king_square == chess.G8:\n",
    "                pawn_shield_squares = [chess.F7, chess.G7, chess.H7]\n",
    "            elif king_square == chess.C8:\n",
    "                pawn_shield_squares = [chess.A7, chess.B7, chess.C7]\n",
    "            else:\n",
    "                pawn_shield_squares = [chess.D7, chess.E7, chess.F7]\n",
    "\n",
    "        for square in pawn_shield_squares:\n",
    "            piece = board.piece_at(square)\n",
    "            if piece and piece.piece_type == chess.PAWN and piece.color == color:\n",
    "                safety_score += 0.5\n",
    "            else:\n",
    "                safety_score -= 0.5\n",
    "\n",
    "        return safety_score\n",
    "\n",
    "    white_king_safety = king_safety_for_color(chess.WHITE)\n",
    "    black_king_safety = king_safety_for_color(chess.BLACK)\n",
    "    return white_king_safety - black_king_safety\n",
    "\n",
    "# CALCULATED BUT UNUSED\n",
    "def get_game_phase(turn_number):\n",
    "    \"\"\"\n",
    "    Determine the game phase based on the turn number.\n",
    "\n",
    "    Args:\n",
    "        turn_number (int): The current turn number.\n",
    "\n",
    "    Returns:\n",
    "        dict: One-hot encoded game phase.\n",
    "    \"\"\"\n",
    "    if turn_number <= 15:\n",
    "        return {'opening': 1, 'middle_game': 0, 'endgame': 0}\n",
    "    elif turn_number <= 40:\n",
    "        return {'opening': 0, 'middle_game': 1, 'endgame': 0}\n",
    "    else:\n",
    "        return {'opening': 0, 'middle_game': 0, 'endgame': 1}\n",
    "\n",
    "def calculate_pawn_structure(board):\n",
    "    \"\"\"\n",
    "    Calculate pawn structure metrics like doubled and isolated pawns.\n",
    "\n",
    "    Args:\n",
    "        board (chess.Board): The current game state.\n",
    "\n",
    "    Returns:\n",
    "        dict: Differences in pawn structure metrics (white - black).\n",
    "    \"\"\"\n",
    "    def pawn_structure_for_color(color):\n",
    "        pawns = board.pieces(chess.PAWN, color)\n",
    "        files_with_pawns = [chess.square_file(square) for square in pawns]\n",
    "        file_counts = Counter(files_with_pawns)\n",
    "\n",
    "        # Doubled pawns: files with more than one pawn\n",
    "        doubled_pawns = sum(1 for count in file_counts.values() if count > 1)\n",
    "\n",
    "        # Isolated pawns: pawns with no friendly pawns on adjacent files\n",
    "        isolated_pawns = 0\n",
    "        for file in file_counts:\n",
    "            adjacent_files = [file - 1, file + 1]\n",
    "            has_adjacent_pawn = any(\n",
    "                adj_file in file_counts for adj_file in adjacent_files if 0 <= adj_file <= 7\n",
    "            )\n",
    "            if not has_adjacent_pawn:\n",
    "                isolated_pawns += file_counts[file]\n",
    "\n",
    "        return {'doubled_pawns': doubled_pawns, 'isolated_pawns': isolated_pawns}\n",
    "\n",
    "    white_pawn_structure = pawn_structure_for_color(chess.WHITE)\n",
    "    black_pawn_structure = pawn_structure_for_color(chess.BLACK)\n",
    "\n",
    "    pawn_structure_diff = {\n",
    "        'doubled_pawns_diff': white_pawn_structure['doubled_pawns'] - black_pawn_structure['doubled_pawns'],\n",
    "        'isolated_pawns_diff': white_pawn_structure['isolated_pawns'] - black_pawn_structure['isolated_pawns']\n",
    "    }\n",
    "    return pawn_structure_diff\n",
    "\n",
    "def calculate_control_of_key_squares(board):\n",
    "    \"\"\"\n",
    "    Calculate the difference in control of key squares.\n",
    "\n",
    "    Args:\n",
    "        board (chess.Board): The current game state.\n",
    "\n",
    "    Returns:\n",
    "        int: Difference in control of key squares (white - black).\n",
    "    \"\"\"\n",
    "    key_squares = [chess.E4, chess.D4, chess.E5, chess.D5]\n",
    "    white_control = sum(1 for square in key_squares if board.is_attacked_by(chess.WHITE, square))\n",
    "    black_control = sum(1 for square in key_squares if board.is_attacked_by(chess.BLACK, square))\n",
    "    return white_control - black_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UJuqrAtxGFDa",
    "outputId": "850c0562-4757-442f-bc75-e7ef11ed1c4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20058 games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing games: 100%|██████████| 20058/20058 [10:27<00:00, 31.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed game states saved to processed_game_states.csv\n",
      "   turn_number                                                fen  piece_diff  \\\n",
      "0            1  rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR ...           0   \n",
      "1            2  rnbqkbnr/ppp1pppp/8/3p4/3P4/8/PPP1PPPP/RNBQKBN...           0   \n",
      "2            3  rnbqkbnr/ppp1pppp/8/3p4/2PP4/8/PP2PPPP/RNBQKBN...           0   \n",
      "3            4  rnbqkbnr/pp2pppp/2p5/3p4/2PP4/8/PP2PPPP/RNBQKB...           0   \n",
      "4            5  rnbqkbnr/pp2pppp/2p5/3P4/3P4/8/PP2PPPP/RNBQKBN...           1   \n",
      "\n",
      "   elo_diff  mobility  king_safety  control_of_key_squares winner         y  \\\n",
      "0       309         8         -1.0                       2  white  0.538462   \n",
      "1       309         0          0.0                       0  white  0.576923   \n",
      "2       309         2          0.0                       1  white  0.615385   \n",
      "3       309         1          0.0                       1  white  0.653846   \n",
      "4       309         0          0.0                       1  white  0.692308   \n",
      "\n",
      "   opening  middle_game  endgame  doubled_pawns_diff  isolated_pawns_diff  \n",
      "0        1            0        0                   0                    0  \n",
      "1        1            0        0                   0                    0  \n",
      "2        1            0        0                   0                    0  \n",
      "3        1            0        0                   0                    0  \n",
      "4        1            0        0                   1                    0  \n"
     ]
    }
   ],
   "source": [
    "if chess_df is not None:\n",
    "    # Process the data\n",
    "    game_states = process_chess_data(chess_df)\n",
    "\n",
    "    # Save or inspect the results\n",
    "    output_file = \"processed_game_states.csv\"\n",
    "    game_states.to_csv(output_file, index=False)\n",
    "    print(f\"Processed game states saved to {output_file}\")\n",
    "    print(game_states.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 1000 positions\n",
      "Evaluated 2000 positions\n",
      "Evaluated 3000 positions\n",
      "Evaluated 4000 positions\n",
      "Evaluated 5000 positions\n",
      "Evaluated 6000 positions\n",
      "Evaluated 7000 positions\n",
      "Evaluated 8000 positions\n",
      "Evaluated 9000 positions\n",
      "Evaluated 10000 positions\n",
      "Evaluated 11000 positions\n",
      "Evaluated 12000 positions\n",
      "Evaluated 13000 positions\n",
      "Evaluated 14000 positions\n",
      "Evaluated 15000 positions\n",
      "Evaluated 16000 positions\n",
      "Evaluated 17000 positions\n",
      "Evaluated 18000 positions\n",
      "Evaluated 19000 positions\n",
      "Evaluated 20000 positions\n",
      "Evaluated 21000 positions\n",
      "Evaluated 22000 positions\n",
      "Evaluated 23000 positions\n",
      "Evaluated 24000 positions\n",
      "Evaluated 25000 positions\n",
      "Evaluated 26000 positions\n",
      "Evaluated 27000 positions\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('')\n",
    "import chess\n",
    "import chess.engine\n",
    "import math\n",
    "import pandas as pd\n",
    "import csv\n",
    "# Path to the Stockfish binary\n",
    "stockfish_path = \"stockfish/stockfish-ubuntu-x86-64-avx2\"\n",
    "\n",
    "# Function to convert centipawn evaluation to win probability for White\n",
    "def evaluation_to_probability(eval_centipawns):\n",
    "    k = 0.004  # Scaling factor\n",
    "    return 1 / (1 + math.exp(-k * eval_centipawns))\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"processed_game_states.csv\"\n",
    "game_states = pd.read_csv(file_path)\n",
    "\n",
    "# Start the Stockfish engine\n",
    "# Function to convert centipawn evaluation to win probability for White\n",
    "def evaluation_to_probability(eval_centipawns):\n",
    "    k = 0.004  # Scaling factor\n",
    "    return 1 / (1 + math.exp(-k * eval_centipawns))\n",
    "\n",
    "# Path to the CSV file\n",
    "file_path = \"processed_game_states.csv\"\n",
    "\n",
    "# Open Stockfish engine\n",
    "i = 0\n",
    "with chess.engine.SimpleEngine.popen_uci(stockfish_path) as engine:\n",
    "    with open(file_path, 'r+', newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        fieldnames = reader.fieldnames\n",
    "        \n",
    "        # Ensure y_gt is a field\n",
    "        if 'y_gt' not in fieldnames:\n",
    "            fieldnames.append('y_gt')\n",
    "        \n",
    "        # Rewind file to write updated rows\n",
    "        rows = list(reader)\n",
    "        csvfile.seek(0)\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for row in rows:\n",
    "            if 'y_gt' in row and row['y_gt']:\n",
    "                # Skip already evaluated rows\n",
    "                writer.writerow(row)\n",
    "                continue\n",
    "            \n",
    "            fen = row['fen']\n",
    "            board = chess.Board(fen)\n",
    "            \n",
    "            # Get evaluation from Stockfish\n",
    "            try:\n",
    "                info = engine.analyse(board, chess.engine.Limit(time=0.1))\n",
    "                evaluation = info[\"score\"].white()\n",
    "\n",
    "                if evaluation.is_mate():\n",
    "                    win_probability = 1.0 if evaluation.mate() > 0 else 0.0\n",
    "                else:\n",
    "                    eval_centipawns = evaluation.score()\n",
    "                    win_probability = evaluation_to_probability(eval_centipawns)\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating FEN: {fen}, {e}\")\n",
    "                win_probability = None\n",
    "            \n",
    "            # Update the row with the new probability\n",
    "            row['y_gt'] = win_probability\n",
    "            writer.writerow(row)\n",
    "            \n",
    "            i += 1\n",
    "            if i % 1000 == 0:\n",
    "                print(f'Evaluated {i} positions')\n",
    "\n",
    "print(f\"CSV file updated with win probabilities.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5DofSEng6f6"
   },
   "source": [
    "# Part 3: Train the model\n",
    "\n",
    "We use our own CNN model to train the data. We first transform the board into a tensor, then normalize the above parameters to enhance the CNN structure. \n",
    "\n",
    "### Output Data\n",
    "One of the main problems with our idea is that we don't have a definitive prediction of a game's win probability status for every single game state - we are only given the eventual winner for training. Thus, after research and experimenting, we devised a formula, mostly linear, based on the final win party and game state, to assign a win probability to each state. Our model is now aimed to produce this prediction based on the above parameters with no knowledge of the winner.\n",
    "\n",
    "The training history is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('')\n",
    "import chess\n",
    "import numpy as np\n",
    "\n",
    "def fen_to_tensor(fen):\n",
    "    \"\"\"\n",
    "    Convert a FEN string into a 8x8x13 numpy array.\n",
    "    If you wish to use symmetrical indexing, adjust piece_to_channel accordingly.\n",
    "    \"\"\"\n",
    "    piece_to_channel = {\n",
    "        'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5,\n",
    "        'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11\n",
    "    }\n",
    "    \n",
    "    board = chess.Board(fen)\n",
    "    board_tensor = np.zeros((8, 8, 13), dtype=np.float32)\n",
    "\n",
    "    for square in chess.SQUARES:\n",
    "        piece = board.piece_at(square)\n",
    "        rank = 7 - (square // 8)\n",
    "        file = square % 8\n",
    "        if piece:\n",
    "            ch = piece_to_channel[piece.symbol()]\n",
    "            board_tensor[rank, file, ch] = 1.0\n",
    "        else:\n",
    "            board_tensor[rank, file, 12] = 1.0\n",
    "\n",
    "    return board_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting FENs to tensors: 100%|██████████| 1032831/1032831 [01:55<00:00, 8907.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the loaded tensors: (1032831, 8, 8, 13)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('')\n",
    "import chess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def fen_to_tensor(fen):\n",
    "    \"\"\"\n",
    "    Convert a FEN string into a 8x8x13 numpy array.\n",
    "    If you wish to use symmetrical indexing, adjust piece_to_channel accordingly.\n",
    "    \"\"\"\n",
    "    piece_to_channel = {\n",
    "        'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5,\n",
    "        'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11\n",
    "    }\n",
    "    \n",
    "    board = chess.Board(fen)\n",
    "    board_tensor = np.zeros((8, 8, 13), dtype=np.float32)\n",
    "\n",
    "    for square in chess.SQUARES:\n",
    "        piece = board.piece_at(square)\n",
    "        rank = 7 - (square // 8)\n",
    "        file = square % 8\n",
    "        if piece:\n",
    "            ch = piece_to_channel[piece.symbol()]\n",
    "            board_tensor[rank, file, ch] = 1.0\n",
    "        else:\n",
    "            board_tensor[rank, file, 12] = 1.0\n",
    "\n",
    "    return board_tensor\n",
    "\n",
    "def load_fen_tensors_from_csv(csv_path, fen_column=\"FEN\", limit=None):\n",
    "    \"\"\"\n",
    "    Load FEN strings from a CSV and convert them to tensors.\n",
    "    \n",
    "    Args:\n",
    "        csv_path (str): Path to the CSV file.\n",
    "        fen_column (str): Name of the column containing FEN strings.\n",
    "        limit (int): Optional limit on the number of rows to process.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: A numpy array of shape (num_positions, 8, 8, 13).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if limit is not None:\n",
    "        df = df.head(limit)\n",
    "    \n",
    "    tensors = []\n",
    "    for fen in tqdm(df[fen_column], desc=\"Converting FENs to tensors\"):\n",
    "        tensor = fen_to_tensor(fen)\n",
    "        tensors.append(tensor)\n",
    "    \n",
    "    # Convert list to NumPy array\n",
    "    # Shape will be (N, 8, 8, 13) where N is the number of rows processed\n",
    "    return np.stack(tensors, axis=0)\n",
    "\n",
    "board_tensors = load_fen_tensors_from_csv(\"processed_game_states.csv\", fen_column=\"fen\")\n",
    "print(\"Shape of the loaded tensors:\", board_tensors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra features shape: (1032831, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"processed_game_states.csv\")\n",
    "\n",
    "feature_columns = [\n",
    "    'piece_diff',\n",
    "    'mobility',\n",
    "    'king_safety',\n",
    "    'control_of_key_squares',\n",
    "    'doubled_pawns_diff',\n",
    "    'isolated_pawns_diff'\n",
    "]\n",
    "\n",
    "# Extract the features into a NumPy array\n",
    "extra_features = df[feature_columns].values.astype(np.float32)\n",
    "# extra_features shape: (N, 6)\n",
    "\n",
    "print(\"Extra features shape:\", extra_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth shape: (1032831, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV into a DataFrame\n",
    "df = pd.read_csv(\"processed_game_states.csv\")\n",
    "\n",
    "gt_column = [\n",
    "    'y_gt'\n",
    "]\n",
    "\n",
    "y_gt = df[gt_column].values.astype(np.float32)\n",
    "\n",
    "print(\"Ground truth shape:\", y_gt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PmswprK8Sx8p",
    "outputId": "be451fe8-90b2-4278-efb9-eb9f0dbe0d5d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 16:39:52.113826: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-12 16:39:52.757472: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 1032831\n",
      "Training samples: 929547\n",
      "Validation samples: 103284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 16:39:57.174009: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " board_input (InputLayer)    [(None, 8, 8, 13)]           0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 8, 8, 64)             7552      ['board_input[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 8, 8, 64)             256       ['conv2d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 8, 8, 64)             36928     ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 8, 8, 64)             256       ['conv2d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 4, 4, 64)             0         ['batch_normalization_1[0][0]'\n",
      " D)                                                                 ]                             \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 128)            73856     ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 4, 4, 128)            512       ['conv2d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 4, 4, 128)            147584    ['batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 4, 4, 128)            512       ['conv2d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " extra_input (InputLayer)    [(None, 6)]                  0         []                            \n",
      "                                                                                                  \n",
      " global_average_pooling2d (  (None, 128)                  0         ['batch_normalization_3[0][0]'\n",
      " GlobalAveragePooling2D)                                            ]                             \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 64)                   448       ['extra_input[0][0]']         \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 128)                  0         ['global_average_pooling2d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 64)                   256       ['dense_1[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 256)                  33024     ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 32)                   2080      ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 256)                  1024      ['dense[0][0]']               \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 32)                   128       ['dense_2[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 256)                  0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 32)                   0         ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 288)                  0         ['dropout_1[0][0]',           \n",
      "                                                                     'dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 128)                  36992     ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 128)                  512       ['dense_3[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 128)                  0         ['batch_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 64)                   8256      ['dropout_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 64)                   256       ['dense_4[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 64)                   0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 1)                    65        ['dropout_4[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 350497 (1.34 MB)\n",
      "Trainable params: 348641 (1.33 MB)\n",
      "Non-trainable params: 1856 (7.25 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, Input\n",
    "import csv\n",
    "import os\n",
    "\n",
    "N = board_tensors.shape[0]\n",
    "print(\"Total samples:\", N)\n",
    "\n",
    "# Split into train/validation\n",
    "board_train, board_val, extra_train, extra_val, y_train, y_val = train_test_split(\n",
    "    board_tensors, extra_features, y_gt, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training samples:\", board_train.shape[0])\n",
    "print(\"Validation samples:\", board_val.shape[0])\n",
    "\n",
    "# Define the model\n",
    "def create_model():\n",
    "    # Board input branch\n",
    "    board_input = Input(shape=(8, 8, 13), name=\"board_input\")\n",
    "\n",
    "    # Convolutional block 1\n",
    "    x = layers.Conv2D(64, (3,3), padding='same', activation='relu')(board_input)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(64, (3,3), padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2,2))(x)  # reduces spatial dimension from 8x8 to 4x4\n",
    "\n",
    "    # Convolutional block 2\n",
    "    x = layers.Conv2D(128, (3,3), padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(128, (3,3), padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    # Global pooling instead of flatten: reduces parameters and can generalize better\n",
    "    x = layers.GlobalAveragePooling2D()(x)  # outputs shape (None, 128)\n",
    "    x = layers.Dropout(0.3)(x)  # dropout for regularization\n",
    "\n",
    "    # Dense layer to further process board features\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    # Extra features input branch\n",
    "    extra_input = Input(shape=(6,), name=\"extra_input\")\n",
    "    y = layers.Dense(64, activation='relu')(extra_input)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Dense(32, activation='relu')(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Dropout(0.2)(y)\n",
    "\n",
    "    # Combine both branches\n",
    "    combined = layers.Concatenate()([x, y])\n",
    "\n",
    "    # Dense layers after concatenation\n",
    "    combined = layers.Dense(128, activation='relu')(combined)\n",
    "    combined = layers.BatchNormalization()(combined)\n",
    "    combined = layers.Dropout(0.3)(combined)\n",
    "\n",
    "    combined = layers.Dense(64, activation='relu')(combined)\n",
    "    combined = layers.BatchNormalization()(combined)\n",
    "    combined = layers.Dropout(0.3)(combined)\n",
    "\n",
    "    # Final output layer - using sigmoid if the target is a probability in [0,1]\n",
    "    output = layers.Dense(1, activation='sigmoid')(combined)\n",
    "\n",
    "    model = models.Model(inputs=[board_input, extra_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "14525/14525 [==============================] - ETA: 0s - loss: 0.0281 - mae: 0.1124\n",
      "Epoch 1: val_loss improved from inf to 0.02285, saving model to models_7/best_model.h5\n",
      "14525/14525 [==============================] - 169s 11ms/step - loss: 0.0281 - mae: 0.1124 - val_loss: 0.0228 - val_mae: 0.0939 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "    6/14525 [..............................] - ETA: 2:48 - loss: 0.0265 - mae: 0.1110"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14525/14525 [==============================] - ETA: 0s - loss: 0.0234 - mae: 0.1004\n",
      "Epoch 2: val_loss improved from 0.02285 to 0.02144, saving model to models_7/best_model.h5\n",
      "14525/14525 [==============================] - 165s 11ms/step - loss: 0.0234 - mae: 0.1004 - val_loss: 0.0214 - val_mae: 0.0973 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0213 - mae: 0.0954\n",
      "Epoch 3: val_loss improved from 0.02144 to 0.01950, saving model to models_7/best_model.h5\n",
      "14525/14525 [==============================] - 166s 11ms/step - loss: 0.0213 - mae: 0.0954 - val_loss: 0.0195 - val_mae: 0.0904 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0199 - mae: 0.0921\n",
      "Epoch 4: val_loss improved from 0.01950 to 0.01910, saving model to models_7/best_model.h5\n",
      "14525/14525 [==============================] - 165s 11ms/step - loss: 0.0199 - mae: 0.0921 - val_loss: 0.0191 - val_mae: 0.0876 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0189 - mae: 0.0897\n",
      "Epoch 5: val_loss improved from 0.01910 to 0.01794, saving model to models_7/best_model.h5\n",
      "14525/14525 [==============================] - 165s 11ms/step - loss: 0.0189 - mae: 0.0897 - val_loss: 0.0179 - val_mae: 0.0854 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "14524/14525 [============================>.] - ETA: 0s - loss: 0.0180 - mae: 0.0876\n",
      "Epoch 6: val_loss did not improve from 0.01794\n",
      "14525/14525 [==============================] - 164s 11ms/step - loss: 0.0180 - mae: 0.0876 - val_loss: 0.0180 - val_mae: 0.0851 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0172 - mae: 0.0859\n",
      "Epoch 7: val_loss improved from 0.01794 to 0.01782, saving model to models_7/best_model.h5\n",
      "14525/14525 [==============================] - 164s 11ms/step - loss: 0.0172 - mae: 0.0859 - val_loss: 0.0178 - val_mae: 0.0847 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0166 - mae: 0.0843\n",
      "Epoch 8: val_loss improved from 0.01782 to 0.01699, saving model to models_7/best_model.h5\n",
      "14525/14525 [==============================] - 163s 11ms/step - loss: 0.0166 - mae: 0.0843 - val_loss: 0.0170 - val_mae: 0.0801 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0161 - mae: 0.0832\n",
      "Epoch 9: val_loss improved from 0.01699 to 0.01658, saving model to models_7/best_model.h5\n",
      "14525/14525 [==============================] - 163s 11ms/step - loss: 0.0161 - mae: 0.0832 - val_loss: 0.0166 - val_mae: 0.0806 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "14525/14525 [==============================] - ETA: 0s - loss: 0.0155 - mae: 0.0816\n",
      "Epoch 10: val_loss did not improve from 0.01658\n",
      "14525/14525 [==============================] - 163s 11ms/step - loss: 0.0155 - mae: 0.0816 - val_loss: 0.0166 - val_mae: 0.0805 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0151 - mae: 0.0808\n",
      "Epoch 11: val_loss improved from 0.01658 to 0.01619, saving model to models_7/best_model.h5\n",
      "14525/14525 [==============================] - 163s 11ms/step - loss: 0.0151 - mae: 0.0808 - val_loss: 0.0162 - val_mae: 0.0780 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0147 - mae: 0.0797\n",
      "Epoch 12: val_loss did not improve from 0.01619\n",
      "14525/14525 [==============================] - 164s 11ms/step - loss: 0.0148 - mae: 0.0797 - val_loss: 0.0168 - val_mae: 0.0782 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "14525/14525 [==============================] - ETA: 0s - loss: 0.0144 - mae: 0.0784\n",
      "Epoch 13: val_loss improved from 0.01619 to 0.01589, saving model to models_7/best_model.h5\n",
      "14525/14525 [==============================] - 163s 11ms/step - loss: 0.0144 - mae: 0.0784 - val_loss: 0.0159 - val_mae: 0.0760 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0141 - mae: 0.0773\n",
      "Epoch 14: val_loss did not improve from 0.01589\n",
      "14525/14525 [==============================] - 163s 11ms/step - loss: 0.0141 - mae: 0.0773 - val_loss: 0.0170 - val_mae: 0.0803 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0137 - mae: 0.0761\n",
      "Epoch 15: val_loss improved from 0.01589 to 0.01577, saving model to models_7/best_model.h5\n",
      "14525/14525 [==============================] - 165s 11ms/step - loss: 0.0137 - mae: 0.0761 - val_loss: 0.0158 - val_mae: 0.0767 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0135 - mae: 0.0754\n",
      "Epoch 16: val_loss did not improve from 0.01577\n",
      "14525/14525 [==============================] - 165s 11ms/step - loss: 0.0135 - mae: 0.0754 - val_loss: 0.0159 - val_mae: 0.0768 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "14525/14525 [==============================] - ETA: 0s - loss: 0.0132 - mae: 0.0747\n",
      "Epoch 17: val_loss did not improve from 0.01577\n",
      "14525/14525 [==============================] - 166s 11ms/step - loss: 0.0132 - mae: 0.0747 - val_loss: 0.0162 - val_mae: 0.0784 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0129 - mae: 0.0741\n",
      "Epoch 18: val_loss did not improve from 0.01577\n",
      "14525/14525 [==============================] - 166s 11ms/step - loss: 0.0129 - mae: 0.0741 - val_loss: 0.0158 - val_mae: 0.0772 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "14523/14525 [============================>.] - ETA: 0s - loss: 0.0127 - mae: 0.0733\n",
      "Epoch 19: val_loss did not improve from 0.01577\n",
      "14525/14525 [==============================] - 164s 11ms/step - loss: 0.0127 - mae: 0.0733 - val_loss: 0.0158 - val_mae: 0.0758 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "14525/14525 [==============================] - ETA: 0s - loss: 0.0125 - mae: 0.0727\n",
      "Epoch 20: val_loss did not improve from 0.01577\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "14525/14525 [==============================] - 164s 11ms/step - loss: 0.0125 - mae: 0.0727 - val_loss: 0.0158 - val_mae: 0.0785 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0113 - mae: 0.0694\n",
      "Epoch 21: val_loss improved from 0.01577 to 0.01474, saving model to models_7/best_model.h5\n",
      "14525/14525 [==============================] - 164s 11ms/step - loss: 0.0113 - mae: 0.0694 - val_loss: 0.0147 - val_mae: 0.0710 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "14525/14525 [==============================] - ETA: 0s - loss: 0.0109 - mae: 0.0683\n",
      "Epoch 22: val_loss improved from 0.01474 to 0.01473, saving model to models_7/best_model.h5\n",
      "14525/14525 [==============================] - 163s 11ms/step - loss: 0.0109 - mae: 0.0683 - val_loss: 0.0147 - val_mae: 0.0714 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "14525/14525 [==============================] - ETA: 0s - loss: 0.0106 - mae: 0.0674\n",
      "Epoch 23: val_loss improved from 0.01473 to 0.01467, saving model to models_7/best_model.h5\n",
      "14525/14525 [==============================] - 164s 11ms/step - loss: 0.0106 - mae: 0.0674 - val_loss: 0.0147 - val_mae: 0.0709 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0104 - mae: 0.0671\n",
      "Epoch 24: val_loss improved from 0.01467 to 0.01462, saving model to models_7/best_model.h5\n",
      "14525/14525 [==============================] - 163s 11ms/step - loss: 0.0104 - mae: 0.0671 - val_loss: 0.0146 - val_mae: 0.0713 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "14525/14525 [==============================] - ETA: 0s - loss: 0.0103 - mae: 0.0667\n",
      "Epoch 25: val_loss did not improve from 0.01462\n",
      "14525/14525 [==============================] - 163s 11ms/step - loss: 0.0103 - mae: 0.0667 - val_loss: 0.0147 - val_mae: 0.0718 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0102 - mae: 0.0663\n",
      "Epoch 26: val_loss improved from 0.01462 to 0.01461, saving model to models_7/best_model.h5\n",
      "14525/14525 [==============================] - 163s 11ms/step - loss: 0.0102 - mae: 0.0663 - val_loss: 0.0146 - val_mae: 0.0719 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0101 - mae: 0.0660\n",
      "Epoch 27: val_loss did not improve from 0.01461\n",
      "14525/14525 [==============================] - 162s 11ms/step - loss: 0.0101 - mae: 0.0660 - val_loss: 0.0147 - val_mae: 0.0710 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0100 - mae: 0.0658\n",
      "Epoch 28: val_loss did not improve from 0.01461\n",
      "14525/14525 [==============================] - 162s 11ms/step - loss: 0.0100 - mae: 0.0658 - val_loss: 0.0148 - val_mae: 0.0715 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0099 - mae: 0.0654\n",
      "Epoch 29: val_loss improved from 0.01461 to 0.01456, saving model to models_7/best_model.h5\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "14525/14525 [==============================] - 163s 11ms/step - loss: 0.0099 - mae: 0.0654 - val_loss: 0.0146 - val_mae: 0.0704 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0093 - mae: 0.0637\n",
      "Epoch 30: val_loss improved from 0.01456 to 0.01437, saving model to models_7/best_model.h5\n",
      "14525/14525 [==============================] - 162s 11ms/step - loss: 0.0093 - mae: 0.0637 - val_loss: 0.0144 - val_mae: 0.0694 - lr: 2.5000e-04\n",
      "Epoch 31/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0091 - mae: 0.0630\n",
      "Epoch 31: val_loss did not improve from 0.01437\n",
      "14525/14525 [==============================] - 162s 11ms/step - loss: 0.0091 - mae: 0.0630 - val_loss: 0.0144 - val_mae: 0.0698 - lr: 2.5000e-04\n",
      "Epoch 32/100\n",
      "14525/14525 [==============================] - ETA: 0s - loss: 0.0090 - mae: 0.0627\n",
      "Epoch 32: val_loss did not improve from 0.01437\n",
      "14525/14525 [==============================] - 162s 11ms/step - loss: 0.0090 - mae: 0.0627 - val_loss: 0.0145 - val_mae: 0.0694 - lr: 2.5000e-04\n",
      "Epoch 33/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0089 - mae: 0.0624\n",
      "Epoch 33: val_loss did not improve from 0.01437\n",
      "14525/14525 [==============================] - 162s 11ms/step - loss: 0.0089 - mae: 0.0624 - val_loss: 0.0144 - val_mae: 0.0687 - lr: 2.5000e-04\n",
      "Epoch 34/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0088 - mae: 0.0622\n",
      "Epoch 34: val_loss did not improve from 0.01437\n",
      "14525/14525 [==============================] - 162s 11ms/step - loss: 0.0088 - mae: 0.0622 - val_loss: 0.0144 - val_mae: 0.0691 - lr: 2.5000e-04\n",
      "Epoch 35/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0087 - mae: 0.0619\n",
      "Epoch 35: val_loss did not improve from 0.01437\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "14525/14525 [==============================] - 162s 11ms/step - loss: 0.0087 - mae: 0.0619 - val_loss: 0.0145 - val_mae: 0.0694 - lr: 2.5000e-04\n",
      "Epoch 36/100\n",
      "14525/14525 [==============================] - ETA: 0s - loss: 0.0085 - mae: 0.0610\n",
      "Epoch 36: val_loss improved from 0.01437 to 0.01436, saving model to models_7/best_model.h5\n",
      "14525/14525 [==============================] - 163s 11ms/step - loss: 0.0085 - mae: 0.0610 - val_loss: 0.0144 - val_mae: 0.0691 - lr: 1.2500e-04\n",
      "Epoch 37/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0084 - mae: 0.0607\n",
      "Epoch 37: val_loss did not improve from 0.01436\n",
      "14525/14525 [==============================] - 162s 11ms/step - loss: 0.0084 - mae: 0.0607 - val_loss: 0.0144 - val_mae: 0.0696 - lr: 1.2500e-04\n",
      "Epoch 38/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0083 - mae: 0.0606\n",
      "Epoch 38: val_loss improved from 0.01436 to 0.01432, saving model to models_7/best_model.h5\n",
      "14525/14525 [==============================] - 162s 11ms/step - loss: 0.0083 - mae: 0.0606 - val_loss: 0.0143 - val_mae: 0.0688 - lr: 1.2500e-04\n",
      "Epoch 39/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0083 - mae: 0.0605\n",
      "Epoch 39: val_loss did not improve from 0.01432\n",
      "14525/14525 [==============================] - 163s 11ms/step - loss: 0.0083 - mae: 0.0605 - val_loss: 0.0143 - val_mae: 0.0685 - lr: 1.2500e-04\n",
      "Epoch 40/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0082 - mae: 0.0603\n",
      "Epoch 40: val_loss improved from 0.01432 to 0.01431, saving model to models_7/best_model.h5\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "14525/14525 [==============================] - 162s 11ms/step - loss: 0.0082 - mae: 0.0603 - val_loss: 0.0143 - val_mae: 0.0687 - lr: 1.2500e-04\n",
      "Epoch 41/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0081 - mae: 0.0599\n",
      "Epoch 41: val_loss did not improve from 0.01431\n",
      "14525/14525 [==============================] - 162s 11ms/step - loss: 0.0081 - mae: 0.0599 - val_loss: 0.0144 - val_mae: 0.0685 - lr: 6.2500e-05\n",
      "Epoch 42/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0081 - mae: 0.0597\n",
      "Epoch 42: val_loss improved from 0.01431 to 0.01430, saving model to models_7/best_model.h5\n",
      "14525/14525 [==============================] - 162s 11ms/step - loss: 0.0081 - mae: 0.0597 - val_loss: 0.0143 - val_mae: 0.0684 - lr: 6.2500e-05\n",
      "Epoch 43/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0081 - mae: 0.0598\n",
      "Epoch 43: val_loss improved from 0.01430 to 0.01430, saving model to models_7/best_model.h5\n",
      "14525/14525 [==============================] - 162s 11ms/step - loss: 0.0081 - mae: 0.0598 - val_loss: 0.0143 - val_mae: 0.0685 - lr: 6.2500e-05\n",
      "Epoch 44/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0080 - mae: 0.0597\n",
      "Epoch 44: val_loss did not improve from 0.01430\n",
      "14525/14525 [==============================] - 161s 11ms/step - loss: 0.0080 - mae: 0.0597 - val_loss: 0.0143 - val_mae: 0.0688 - lr: 6.2500e-05\n",
      "Epoch 45/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0080 - mae: 0.0596\n",
      "Epoch 45: val_loss did not improve from 0.01430\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "14525/14525 [==============================] - 160s 11ms/step - loss: 0.0080 - mae: 0.0596 - val_loss: 0.0143 - val_mae: 0.0691 - lr: 6.2500e-05\n",
      "Epoch 46/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0079 - mae: 0.0593\n",
      "Epoch 46: val_loss improved from 0.01430 to 0.01429, saving model to models_7/best_model.h5\n",
      "14525/14525 [==============================] - 162s 11ms/step - loss: 0.0079 - mae: 0.0593 - val_loss: 0.0143 - val_mae: 0.0684 - lr: 3.1250e-05\n",
      "Epoch 47/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0079 - mae: 0.0593\n",
      "Epoch 47: val_loss did not improve from 0.01429\n",
      "14525/14525 [==============================] - 161s 11ms/step - loss: 0.0079 - mae: 0.0593 - val_loss: 0.0143 - val_mae: 0.0679 - lr: 3.1250e-05\n",
      "Epoch 48/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0079 - mae: 0.0593\n",
      "Epoch 48: val_loss improved from 0.01429 to 0.01428, saving model to models_7/best_model.h5\n",
      "14525/14525 [==============================] - 162s 11ms/step - loss: 0.0079 - mae: 0.0593 - val_loss: 0.0143 - val_mae: 0.0684 - lr: 3.1250e-05\n",
      "Epoch 49/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0079 - mae: 0.0591\n",
      "Epoch 49: val_loss did not improve from 0.01428\n",
      "14525/14525 [==============================] - 163s 11ms/step - loss: 0.0079 - mae: 0.0591 - val_loss: 0.0143 - val_mae: 0.0684 - lr: 3.1250e-05\n",
      "Epoch 50/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0079 - mae: 0.0591\n",
      "Epoch 50: val_loss improved from 0.01428 to 0.01427, saving model to models_7/best_model.h5\n",
      "14525/14525 [==============================] - 163s 11ms/step - loss: 0.0079 - mae: 0.0591 - val_loss: 0.0143 - val_mae: 0.0683 - lr: 3.1250e-05\n",
      "Epoch 51/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0079 - mae: 0.0591\n",
      "Epoch 51: val_loss did not improve from 0.01427\n",
      "14525/14525 [==============================] - 163s 11ms/step - loss: 0.0079 - mae: 0.0591 - val_loss: 0.0143 - val_mae: 0.0685 - lr: 3.1250e-05\n",
      "Epoch 52/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0079 - mae: 0.0592\n",
      "Epoch 52: val_loss did not improve from 0.01427\n",
      "14525/14525 [==============================] - 163s 11ms/step - loss: 0.0079 - mae: 0.0592 - val_loss: 0.0143 - val_mae: 0.0682 - lr: 3.1250e-05\n",
      "Epoch 53/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0079 - mae: 0.0591\n",
      "Epoch 53: val_loss did not improve from 0.01427\n",
      "14525/14525 [==============================] - 162s 11ms/step - loss: 0.0079 - mae: 0.0591 - val_loss: 0.0143 - val_mae: 0.0685 - lr: 3.1250e-05\n",
      "Epoch 54/100\n",
      "14521/14525 [============================>.] - ETA: 0s - loss: 0.0078 - mae: 0.0590\n",
      "Epoch 54: val_loss did not improve from 0.01427\n",
      "14525/14525 [==============================] - 162s 11ms/step - loss: 0.0078 - mae: 0.0590 - val_loss: 0.0143 - val_mae: 0.0683 - lr: 3.1250e-05\n",
      "Epoch 55/100\n",
      " 6911/14525 [=============>................] - ETA: 1:22 - loss: 0.0078 - mae: 0.0589"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Callback for logging progress into CSV\n",
    "class CSVLoggerCustom(callbacks.Callback):\n",
    "    def __init__(self, filename):\n",
    "        super().__init__()\n",
    "        self.filename = filename\n",
    "        self.file = open(filename, 'w', newline='')\n",
    "        self.writer = csv.writer(self.file)\n",
    "        self.writer.writerow([\"epoch\", \"train_error\", \"val_error\"])\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # train_error and val_error here can be the loss or mae. We'll use 'loss' as error.\n",
    "        train_error = logs.get('loss')\n",
    "        val_error = logs.get('val_loss')\n",
    "        self.writer.writerow([epoch+1, train_error, val_error])\n",
    "        self.file.flush()\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.file.close()\n",
    "\n",
    "\n",
    "# Callback for saving every 10 epochs\n",
    "class SaveEvery10Epochs(callbacks.Callback):\n",
    "    def __init__(self, save_prefix=\"final_model/\"):\n",
    "        super().__init__()\n",
    "        self.save_prefix = save_prefix\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            filename = f\"{self.save_prefix}epoch_{epoch+1}.h5\"\n",
    "            self.model.save(filename)\n",
    "\n",
    "\n",
    "# Callback for saving best model based on val_error (val_loss)\n",
    "best_model_filepath = \"final_model/best_model.h5\"\n",
    "checkpoint_best = callbacks.ModelCheckpoint(\n",
    "    filepath=best_model_filepath,\n",
    "    monitor='val_loss',        # validation loss as measure of error\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "csv_logger = CSVLoggerCustom(\"final_model/training_log.csv\")\n",
    "save_every_10 = SaveEvery10Epochs()\n",
    "\n",
    "# Train the model\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                              factor=0.5,       # reduce LR by half\n",
    "                              patience=5,       # wait 5 epochs without improvement\n",
    "                              min_lr=1e-6,      # do not go below this learning rate\n",
    "                              verbose=1)\n",
    "\n",
    "# Train the model with the new callback\n",
    "history = model.fit(\n",
    "    [board_train, extra_train],\n",
    "    y_train,\n",
    "    validation_data=([board_val, extra_val], y_val),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[csv_logger, save_every_10, checkpoint_best, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Analysis\n",
    "\n",
    "The above output cutoff after 55 epochs, but the model continued to train until epoch 80. That's when we decided to stop the training loop as both train and validation error have essentially converged and are no longer changing.\n",
    "\n",
    "The below plot shows the training and validation Mean Squared Error (MSE) over 80 epochs, with a log scale used for the MSE axis.\n",
    "\n",
    "Both the training MSE (blue line) and validation MSE (orange line) decrease significantly during the initial epochs, indicating that the model learns effectively in the early phase.\n",
    "\n",
    "After about 25 epochs, the training MSE continues to decrease smoothly, while the validation MSE starts to stabilize with some fluctuations.\n",
    "\n",
    "After around 60 epochs, the training MSE also seems to converge. \n",
    "\n",
    "Validation loss converged sooner at a slightly higher value, suggesting some overfitting. Despite this, the overall validation performance remains stable.\n",
    "\n",
    "The training MSE converges to a very low value, indicating the model fits the training data well.\n",
    "\n",
    "The validation MSE stabilizes at a slightly higher value compared to the training MSE, which is typical in machine learning due to differences in the distributions of the training and validation datasets.\n",
    "\n",
    "Overall, our best model during the training had a validation loss MSE of 0.01427, which corresponds to around a 6.8% average error when predicting probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_progress(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(df['epoch'], df['train_error'], label='Train Error')\n",
    "    plt.plot(df['epoch'], df['val_error'], label='Validation Error', linestyle='--')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error')\n",
    "    plt.yscale('log')\n",
    "    plt.title('Training and Validation Error Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.savefig(\"final_model/training_progress.png\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_training_progress(\"final_model/training_log.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 5: Model Usage\n",
    "Now, one can use our best model to predict winning percentage (white) given a standard chess board FEN notation.\n",
    "\n",
    "This program can be run from another seperate file by loading the model saved above, or by directly running the cell below. Ensure relevant prior cells are run during the runtime to make sure the methods exist in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input state: r2q1rk1/ppp2ppp/2np1n2/3N4/2P1PB2/3P4/PP3PPP/R2Q1RK1 w - - 0 15\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "\n",
      "Predicted probability of White win: 0.59166646\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" viewBox=\"0 0 390 390\" width=\"350\" height=\"350\"><desc><pre>r . . q . r k .\n",
       "p p p . . p p p\n",
       ". . n p . n . .\n",
       ". . . N . . . .\n",
       ". . P . P B . .\n",
       ". . . P . . . .\n",
       "P P . . . P P P\n",
       "R . . Q . R K .</pre></desc><defs><g id=\"white-pawn\" class=\"white pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#fff\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"white-knight\" class=\"white knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#000000; stroke:#000000;\" /></g><g id=\"white-bishop\" class=\"white bishop\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><g fill=\"#fff\" stroke-linecap=\"butt\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zM15 32c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" /></g><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke-linejoin=\"miter\" /></g><g id=\"white-rook\" class=\"white rook\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12 36v-4h21v4H12zM11 14V9h4v2h5V9h5v2h5V9h4v5\" stroke-linecap=\"butt\" /><path d=\"M34 14l-3 3H14l-3-3\" /><path d=\"M31 17v12.5H14V17\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M31 29.5l1.5 2.5h-20l1.5-2.5\" /><path d=\"M11 14h23\" fill=\"none\" stroke-linejoin=\"miter\" /></g><g id=\"white-queen\" class=\"white queen\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M8 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM24.5 7.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM41 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM16 8.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM33 9a2 2 0 1 1-4 0 2 2 0 1 1 4 0z\" /><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2-12-7 11V11l-5.5 13.5-3-15-3 15-5.5-14V25L7 14l2 12zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11.5 30c3.5-1 18.5-1 22 0M12 33.5c6-1 15-1 21 0\" fill=\"none\" /></g><g id=\"white-king\" class=\"white king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#fff\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#fff\" /><path d=\"M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" /></g><g id=\"black-pawn\" class=\"black pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#000\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"black-knight\" class=\"black knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 24.55,10.4 L 24.1,11.85 L 24.6,12 C 27.75,13 30.25,14.49 32.5,18.75 C 34.75,23.01 35.75,29.06 35.25,39 L 35.2,39.5 L 37.45,39.5 L 37.5,39 C 38,28.94 36.62,22.15 34.25,17.66 C 31.88,13.17 28.46,11.02 25.06,10.5 L 24.55,10.4 z \" style=\"fill:#ececec; stroke:none;\" /></g><g id=\"black-rook\" class=\"black rook\" fill=\"#000\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12.5 32l1.5-2.5h17l1.5 2.5h-20zM12 36v-4h21v4H12z\" stroke-linecap=\"butt\" /><path d=\"M14 29.5v-13h17v13H14z\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M14 16.5L11 14h23l-3 2.5H14zM11 14V9h4v2h5V9h5v2h5V9h4v5H11z\" stroke-linecap=\"butt\" /><path d=\"M12 35.5h21M13 31.5h19M14 29.5h17M14 16.5h17M11 14h23\" fill=\"none\" stroke=\"#fff\" stroke-width=\"1\" stroke-linejoin=\"miter\" /></g><g id=\"black-queen\" class=\"black queen\" fill=\"#000\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><g fill=\"#000\" stroke=\"none\"><circle cx=\"6\" cy=\"12\" r=\"2.75\" /><circle cx=\"14\" cy=\"9\" r=\"2.75\" /><circle cx=\"22.5\" cy=\"8\" r=\"2.75\" /><circle cx=\"31\" cy=\"9\" r=\"2.75\" /><circle cx=\"39\" cy=\"12\" r=\"2.75\" /></g><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2.5-12.5L31 25l-.3-14.1-5.2 13.6-3-14.5-3 14.5-5.2-13.6L14 25 6.5 13.5 9 26zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11 38.5a35 35 1 0 0 23 0\" fill=\"none\" stroke-linecap=\"butt\" /><path d=\"M11 29a35 35 1 0 1 23 0M12.5 31.5h20M11.5 34.5a35 35 1 0 0 22 0M10.5 37.5a35 35 1 0 0 24 0\" fill=\"none\" stroke=\"#fff\" /></g><g id=\"black-king\" class=\"black king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#000\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#000\" /><path d=\"M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M32 29.5s8.5-4 6.03-9.65C34.15 14 25 18 22.5 24.5l.01 2.1-.01-2.1C20 18 9.906 14 6.997 19.85c-2.497 5.65 4.853 9 4.853 9M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" stroke=\"#fff\" /></g></defs><rect x=\"7.5\" y=\"7.5\" width=\"375\" height=\"375\" fill=\"none\" stroke=\"#212121\" stroke-width=\"15\" /><g transform=\"translate(20, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(20, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(65, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(65, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(110, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(110, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(155, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(155, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(200, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(200, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(245, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(245, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(290, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(290, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(335, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(335, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(0, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(375, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(0, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(375, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(0, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(375, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(0, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(375, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(0, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(375, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(0, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(375, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(0, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(375, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(0, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><g transform=\"translate(375, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><rect x=\"15\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark a1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"330\" width=\"45\" height=\"45\" class=\"square light b1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark c1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"330\" width=\"45\" height=\"45\" class=\"square light d1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark e1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"330\" width=\"45\" height=\"45\" class=\"square light f1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark g1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"330\" width=\"45\" height=\"45\" class=\"square light h1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"285\" width=\"45\" height=\"45\" class=\"square light a2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark b2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"285\" width=\"45\" height=\"45\" class=\"square light c2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark d2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"285\" width=\"45\" height=\"45\" class=\"square light e2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark f2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"285\" width=\"45\" height=\"45\" class=\"square light g2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark h2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark a3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"240\" width=\"45\" height=\"45\" class=\"square light b3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark c3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"240\" width=\"45\" height=\"45\" class=\"square light d3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark e3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"240\" width=\"45\" height=\"45\" class=\"square light f3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark g3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"240\" width=\"45\" height=\"45\" class=\"square light h3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"195\" width=\"45\" height=\"45\" class=\"square light a4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark b4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"195\" width=\"45\" height=\"45\" class=\"square light c4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark d4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"195\" width=\"45\" height=\"45\" class=\"square light e4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark f4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"195\" width=\"45\" height=\"45\" class=\"square light g4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark h4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark a5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"150\" width=\"45\" height=\"45\" class=\"square light b5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark c5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"150\" width=\"45\" height=\"45\" class=\"square light d5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark e5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"150\" width=\"45\" height=\"45\" class=\"square light f5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark g5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"150\" width=\"45\" height=\"45\" class=\"square light h5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"105\" width=\"45\" height=\"45\" class=\"square light a6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark b6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"105\" width=\"45\" height=\"45\" class=\"square light c6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark d6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"105\" width=\"45\" height=\"45\" class=\"square light e6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark f6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"105\" width=\"45\" height=\"45\" class=\"square light g6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark h6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark a7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"60\" width=\"45\" height=\"45\" class=\"square light b7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark c7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"60\" width=\"45\" height=\"45\" class=\"square light d7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark e7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"60\" width=\"45\" height=\"45\" class=\"square light f7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark g7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"60\" width=\"45\" height=\"45\" class=\"square light h7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"15\" width=\"45\" height=\"45\" class=\"square light a8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark b8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"15\" width=\"45\" height=\"45\" class=\"square light c8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark d8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"15\" width=\"45\" height=\"45\" class=\"square light e8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark f8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"15\" width=\"45\" height=\"45\" class=\"square light g8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark h8\" stroke=\"none\" fill=\"#d18b47\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(15, 330)\" /><use href=\"#white-queen\" xlink:href=\"#white-queen\" transform=\"translate(150, 330)\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(240, 330)\" /><use href=\"#white-king\" xlink:href=\"#white-king\" transform=\"translate(285, 330)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(15, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(60, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(240, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(285, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(330, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(150, 240)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(105, 195)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(195, 195)\" /><use href=\"#white-bishop\" xlink:href=\"#white-bishop\" transform=\"translate(240, 195)\" /><use href=\"#white-knight\" xlink:href=\"#white-knight\" transform=\"translate(150, 150)\" /><use href=\"#black-knight\" xlink:href=\"#black-knight\" transform=\"translate(105, 105)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(150, 105)\" /><use href=\"#black-knight\" xlink:href=\"#black-knight\" transform=\"translate(240, 105)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(15, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(60, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(105, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(240, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(285, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(330, 60)\" /><use href=\"#black-rook\" xlink:href=\"#black-rook\" transform=\"translate(15, 15)\" /><use href=\"#black-queen\" xlink:href=\"#black-queen\" transform=\"translate(150, 15)\" /><use href=\"#black-rook\" xlink:href=\"#black-rook\" transform=\"translate(240, 15)\" /><use href=\"#black-king\" xlink:href=\"#black-king\" transform=\"translate(285, 15)\" /></svg>"
      ],
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" viewBox=\"0 0 390 390\" width=\"350\" height=\"350\"><desc><pre>r . . q . r k .\n",
       "p p p . . p p p\n",
       ". . n p . n . .\n",
       ". . . N . . . .\n",
       ". . P . P B . .\n",
       ". . . P . . . .\n",
       "P P . . . P P P\n",
       "R . . Q . R K .</pre></desc><defs><g id=\"white-pawn\" class=\"white pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#fff\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"white-knight\" class=\"white knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#000000; stroke:#000000;\" /></g><g id=\"white-bishop\" class=\"white bishop\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><g fill=\"#fff\" stroke-linecap=\"butt\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zM15 32c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" /></g><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke-linejoin=\"miter\" /></g><g id=\"white-rook\" class=\"white rook\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12 36v-4h21v4H12zM11 14V9h4v2h5V9h5v2h5V9h4v5\" stroke-linecap=\"butt\" /><path d=\"M34 14l-3 3H14l-3-3\" /><path d=\"M31 17v12.5H14V17\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M31 29.5l1.5 2.5h-20l1.5-2.5\" /><path d=\"M11 14h23\" fill=\"none\" stroke-linejoin=\"miter\" /></g><g id=\"white-queen\" class=\"white queen\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M8 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM24.5 7.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM41 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM16 8.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM33 9a2 2 0 1 1-4 0 2 2 0 1 1 4 0z\" /><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2-12-7 11V11l-5.5 13.5-3-15-3 15-5.5-14V25L7 14l2 12zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11.5 30c3.5-1 18.5-1 22 0M12 33.5c6-1 15-1 21 0\" fill=\"none\" /></g><g id=\"white-king\" class=\"white king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#fff\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#fff\" /><path d=\"M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" /></g><g id=\"black-pawn\" class=\"black pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#000\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"black-knight\" class=\"black knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 24.55,10.4 L 24.1,11.85 L 24.6,12 C 27.75,13 30.25,14.49 32.5,18.75 C 34.75,23.01 35.75,29.06 35.25,39 L 35.2,39.5 L 37.45,39.5 L 37.5,39 C 38,28.94 36.62,22.15 34.25,17.66 C 31.88,13.17 28.46,11.02 25.06,10.5 L 24.55,10.4 z \" style=\"fill:#ececec; stroke:none;\" /></g><g id=\"black-rook\" class=\"black rook\" fill=\"#000\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12.5 32l1.5-2.5h17l1.5 2.5h-20zM12 36v-4h21v4H12z\" stroke-linecap=\"butt\" /><path d=\"M14 29.5v-13h17v13H14z\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M14 16.5L11 14h23l-3 2.5H14zM11 14V9h4v2h5V9h5v2h5V9h4v5H11z\" stroke-linecap=\"butt\" /><path d=\"M12 35.5h21M13 31.5h19M14 29.5h17M14 16.5h17M11 14h23\" fill=\"none\" stroke=\"#fff\" stroke-width=\"1\" stroke-linejoin=\"miter\" /></g><g id=\"black-queen\" class=\"black queen\" fill=\"#000\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><g fill=\"#000\" stroke=\"none\"><circle cx=\"6\" cy=\"12\" r=\"2.75\" /><circle cx=\"14\" cy=\"9\" r=\"2.75\" /><circle cx=\"22.5\" cy=\"8\" r=\"2.75\" /><circle cx=\"31\" cy=\"9\" r=\"2.75\" /><circle cx=\"39\" cy=\"12\" r=\"2.75\" /></g><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2.5-12.5L31 25l-.3-14.1-5.2 13.6-3-14.5-3 14.5-5.2-13.6L14 25 6.5 13.5 9 26zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11 38.5a35 35 1 0 0 23 0\" fill=\"none\" stroke-linecap=\"butt\" /><path d=\"M11 29a35 35 1 0 1 23 0M12.5 31.5h20M11.5 34.5a35 35 1 0 0 22 0M10.5 37.5a35 35 1 0 0 24 0\" fill=\"none\" stroke=\"#fff\" /></g><g id=\"black-king\" class=\"black king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#000\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#000\" /><path d=\"M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M32 29.5s8.5-4 6.03-9.65C34.15 14 25 18 22.5 24.5l.01 2.1-.01-2.1C20 18 9.906 14 6.997 19.85c-2.497 5.65 4.853 9 4.853 9M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" stroke=\"#fff\" /></g></defs><rect x=\"7.5\" y=\"7.5\" width=\"375\" height=\"375\" fill=\"none\" stroke=\"#212121\" stroke-width=\"15\" /><g transform=\"translate(20, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(20, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(65, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(65, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(110, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(110, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(155, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(155, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(200, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(200, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(245, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(245, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(290, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(290, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(335, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(335, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(0, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(375, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(0, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(375, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(0, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(375, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(0, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(375, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(0, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(375, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(0, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(375, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(0, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(375, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(0, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><g transform=\"translate(375, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><rect x=\"15\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark a1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"330\" width=\"45\" height=\"45\" class=\"square light b1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark c1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"330\" width=\"45\" height=\"45\" class=\"square light d1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark e1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"330\" width=\"45\" height=\"45\" class=\"square light f1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark g1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"330\" width=\"45\" height=\"45\" class=\"square light h1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"285\" width=\"45\" height=\"45\" class=\"square light a2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark b2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"285\" width=\"45\" height=\"45\" class=\"square light c2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark d2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"285\" width=\"45\" height=\"45\" class=\"square light e2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark f2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"285\" width=\"45\" height=\"45\" class=\"square light g2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark h2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark a3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"240\" width=\"45\" height=\"45\" class=\"square light b3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark c3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"240\" width=\"45\" height=\"45\" class=\"square light d3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark e3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"240\" width=\"45\" height=\"45\" class=\"square light f3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark g3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"240\" width=\"45\" height=\"45\" class=\"square light h3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"195\" width=\"45\" height=\"45\" class=\"square light a4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark b4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"195\" width=\"45\" height=\"45\" class=\"square light c4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark d4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"195\" width=\"45\" height=\"45\" class=\"square light e4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark f4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"195\" width=\"45\" height=\"45\" class=\"square light g4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark h4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark a5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"150\" width=\"45\" height=\"45\" class=\"square light b5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark c5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"150\" width=\"45\" height=\"45\" class=\"square light d5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark e5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"150\" width=\"45\" height=\"45\" class=\"square light f5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark g5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"150\" width=\"45\" height=\"45\" class=\"square light h5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"105\" width=\"45\" height=\"45\" class=\"square light a6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark b6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"105\" width=\"45\" height=\"45\" class=\"square light c6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark d6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"105\" width=\"45\" height=\"45\" class=\"square light e6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark f6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"105\" width=\"45\" height=\"45\" class=\"square light g6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark h6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark a7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"60\" width=\"45\" height=\"45\" class=\"square light b7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark c7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"60\" width=\"45\" height=\"45\" class=\"square light d7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark e7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"60\" width=\"45\" height=\"45\" class=\"square light f7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark g7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"60\" width=\"45\" height=\"45\" class=\"square light h7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"15\" width=\"45\" height=\"45\" class=\"square light a8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark b8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"15\" width=\"45\" height=\"45\" class=\"square light c8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark d8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"15\" width=\"45\" height=\"45\" class=\"square light e8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark f8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"15\" width=\"45\" height=\"45\" class=\"square light g8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark h8\" stroke=\"none\" fill=\"#d18b47\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(15, 330)\" /><use href=\"#white-queen\" xlink:href=\"#white-queen\" transform=\"translate(150, 330)\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(240, 330)\" /><use href=\"#white-king\" xlink:href=\"#white-king\" transform=\"translate(285, 330)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(15, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(60, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(240, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(285, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(330, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(150, 240)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(105, 195)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(195, 195)\" /><use href=\"#white-bishop\" xlink:href=\"#white-bishop\" transform=\"translate(240, 195)\" /><use href=\"#white-knight\" xlink:href=\"#white-knight\" transform=\"translate(150, 150)\" /><use href=\"#black-knight\" xlink:href=\"#black-knight\" transform=\"translate(105, 105)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(150, 105)\" /><use href=\"#black-knight\" xlink:href=\"#black-knight\" transform=\"translate(240, 105)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(15, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(60, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(105, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(240, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(285, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(330, 60)\" /><use href=\"#black-rook\" xlink:href=\"#black-rook\" transform=\"translate(15, 15)\" /><use href=\"#black-queen\" xlink:href=\"#black-queen\" transform=\"translate(150, 15)\" /><use href=\"#black-rook\" xlink:href=\"#black-rook\" transform=\"translate(240, 15)\" /><use href=\"#black-king\" xlink:href=\"#black-king\" transform=\"translate(285, 15)\" /></svg>"
      ],
      "text/plain": [
       "'<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" viewBox=\"0 0 390 390\" width=\"350\" height=\"350\"><desc><pre>r . . q . r k .\\np p p . . p p p\\n. . n p . n . .\\n. . . N . . . .\\n. . P . P B . .\\n. . . P . . . .\\nP P . . . P P P\\nR . . Q . R K .</pre></desc><defs><g id=\"white-pawn\" class=\"white pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#fff\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"white-knight\" class=\"white knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#000000; stroke:#000000;\" /></g><g id=\"white-bishop\" class=\"white bishop\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><g fill=\"#fff\" stroke-linecap=\"butt\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zM15 32c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" /></g><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke-linejoin=\"miter\" /></g><g id=\"white-rook\" class=\"white rook\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12 36v-4h21v4H12zM11 14V9h4v2h5V9h5v2h5V9h4v5\" stroke-linecap=\"butt\" /><path d=\"M34 14l-3 3H14l-3-3\" /><path d=\"M31 17v12.5H14V17\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M31 29.5l1.5 2.5h-20l1.5-2.5\" /><path d=\"M11 14h23\" fill=\"none\" stroke-linejoin=\"miter\" /></g><g id=\"white-queen\" class=\"white queen\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M8 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM24.5 7.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM41 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM16 8.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM33 9a2 2 0 1 1-4 0 2 2 0 1 1 4 0z\" /><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2-12-7 11V11l-5.5 13.5-3-15-3 15-5.5-14V25L7 14l2 12zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11.5 30c3.5-1 18.5-1 22 0M12 33.5c6-1 15-1 21 0\" fill=\"none\" /></g><g id=\"white-king\" class=\"white king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#fff\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#fff\" /><path d=\"M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" /></g><g id=\"black-pawn\" class=\"black pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#000\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"black-knight\" class=\"black knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 24.55,10.4 L 24.1,11.85 L 24.6,12 C 27.75,13 30.25,14.49 32.5,18.75 C 34.75,23.01 35.75,29.06 35.25,39 L 35.2,39.5 L 37.45,39.5 L 37.5,39 C 38,28.94 36.62,22.15 34.25,17.66 C 31.88,13.17 28.46,11.02 25.06,10.5 L 24.55,10.4 z \" style=\"fill:#ececec; stroke:none;\" /></g><g id=\"black-rook\" class=\"black rook\" fill=\"#000\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12.5 32l1.5-2.5h17l1.5 2.5h-20zM12 36v-4h21v4H12z\" stroke-linecap=\"butt\" /><path d=\"M14 29.5v-13h17v13H14z\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M14 16.5L11 14h23l-3 2.5H14zM11 14V9h4v2h5V9h5v2h5V9h4v5H11z\" stroke-linecap=\"butt\" /><path d=\"M12 35.5h21M13 31.5h19M14 29.5h17M14 16.5h17M11 14h23\" fill=\"none\" stroke=\"#fff\" stroke-width=\"1\" stroke-linejoin=\"miter\" /></g><g id=\"black-queen\" class=\"black queen\" fill=\"#000\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><g fill=\"#000\" stroke=\"none\"><circle cx=\"6\" cy=\"12\" r=\"2.75\" /><circle cx=\"14\" cy=\"9\" r=\"2.75\" /><circle cx=\"22.5\" cy=\"8\" r=\"2.75\" /><circle cx=\"31\" cy=\"9\" r=\"2.75\" /><circle cx=\"39\" cy=\"12\" r=\"2.75\" /></g><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2.5-12.5L31 25l-.3-14.1-5.2 13.6-3-14.5-3 14.5-5.2-13.6L14 25 6.5 13.5 9 26zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11 38.5a35 35 1 0 0 23 0\" fill=\"none\" stroke-linecap=\"butt\" /><path d=\"M11 29a35 35 1 0 1 23 0M12.5 31.5h20M11.5 34.5a35 35 1 0 0 22 0M10.5 37.5a35 35 1 0 0 24 0\" fill=\"none\" stroke=\"#fff\" /></g><g id=\"black-king\" class=\"black king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#000\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#000\" /><path d=\"M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M32 29.5s8.5-4 6.03-9.65C34.15 14 25 18 22.5 24.5l.01 2.1-.01-2.1C20 18 9.906 14 6.997 19.85c-2.497 5.65 4.853 9 4.853 9M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" stroke=\"#fff\" /></g></defs><rect x=\"7.5\" y=\"7.5\" width=\"375\" height=\"375\" fill=\"none\" stroke=\"#212121\" stroke-width=\"15\" /><g transform=\"translate(20, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(20, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(65, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(65, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(110, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(110, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(155, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(155, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(200, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(200, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(245, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(245, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(290, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(290, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(335, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(335, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(0, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(375, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(0, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(375, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(0, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(375, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(0, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(375, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(0, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(375, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(0, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(375, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(0, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(375, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(0, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><g transform=\"translate(375, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><rect x=\"15\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark a1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"330\" width=\"45\" height=\"45\" class=\"square light b1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark c1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"330\" width=\"45\" height=\"45\" class=\"square light d1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark e1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"330\" width=\"45\" height=\"45\" class=\"square light f1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark g1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"330\" width=\"45\" height=\"45\" class=\"square light h1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"285\" width=\"45\" height=\"45\" class=\"square light a2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark b2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"285\" width=\"45\" height=\"45\" class=\"square light c2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark d2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"285\" width=\"45\" height=\"45\" class=\"square light e2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark f2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"285\" width=\"45\" height=\"45\" class=\"square light g2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark h2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark a3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"240\" width=\"45\" height=\"45\" class=\"square light b3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark c3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"240\" width=\"45\" height=\"45\" class=\"square light d3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark e3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"240\" width=\"45\" height=\"45\" class=\"square light f3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark g3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"240\" width=\"45\" height=\"45\" class=\"square light h3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"195\" width=\"45\" height=\"45\" class=\"square light a4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark b4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"195\" width=\"45\" height=\"45\" class=\"square light c4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark d4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"195\" width=\"45\" height=\"45\" class=\"square light e4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark f4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"195\" width=\"45\" height=\"45\" class=\"square light g4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark h4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark a5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"150\" width=\"45\" height=\"45\" class=\"square light b5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark c5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"150\" width=\"45\" height=\"45\" class=\"square light d5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark e5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"150\" width=\"45\" height=\"45\" class=\"square light f5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark g5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"150\" width=\"45\" height=\"45\" class=\"square light h5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"105\" width=\"45\" height=\"45\" class=\"square light a6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark b6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"105\" width=\"45\" height=\"45\" class=\"square light c6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark d6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"105\" width=\"45\" height=\"45\" class=\"square light e6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark f6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"105\" width=\"45\" height=\"45\" class=\"square light g6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark h6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark a7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"60\" width=\"45\" height=\"45\" class=\"square light b7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark c7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"60\" width=\"45\" height=\"45\" class=\"square light d7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark e7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"60\" width=\"45\" height=\"45\" class=\"square light f7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark g7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"60\" width=\"45\" height=\"45\" class=\"square light h7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"15\" width=\"45\" height=\"45\" class=\"square light a8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark b8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"15\" width=\"45\" height=\"45\" class=\"square light c8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark d8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"15\" width=\"45\" height=\"45\" class=\"square light e8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark f8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"15\" width=\"45\" height=\"45\" class=\"square light g8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark h8\" stroke=\"none\" fill=\"#d18b47\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(15, 330)\" /><use href=\"#white-queen\" xlink:href=\"#white-queen\" transform=\"translate(150, 330)\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(240, 330)\" /><use href=\"#white-king\" xlink:href=\"#white-king\" transform=\"translate(285, 330)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(15, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(60, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(240, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(285, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(330, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(150, 240)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(105, 195)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(195, 195)\" /><use href=\"#white-bishop\" xlink:href=\"#white-bishop\" transform=\"translate(240, 195)\" /><use href=\"#white-knight\" xlink:href=\"#white-knight\" transform=\"translate(150, 150)\" /><use href=\"#black-knight\" xlink:href=\"#black-knight\" transform=\"translate(105, 105)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(150, 105)\" /><use href=\"#black-knight\" xlink:href=\"#black-knight\" transform=\"translate(240, 105)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(15, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(60, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(105, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(240, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(285, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(330, 60)\" /><use href=\"#black-rook\" xlink:href=\"#black-rook\" transform=\"translate(15, 15)\" /><use href=\"#black-queen\" xlink:href=\"#black-queen\" transform=\"translate(150, 15)\" /><use href=\"#black-rook\" xlink:href=\"#black-rook\" transform=\"translate(240, 15)\" /><use href=\"#black-king\" xlink:href=\"#black-king\" transform=\"translate(285, 15)\" /></svg>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('')\n",
    "import chess\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "def predict_from_fen(fen, model_path=\"best_model.h5\"):\n",
    "    # Load the model\n",
    "    model = load_model(model_path)\n",
    "    \n",
    "    # Convert fen to board tensor\n",
    "    board_tensor = fen_to_tensor(fen)\n",
    "    \n",
    "    # Compute extra features\n",
    "    board = chess.Board(fen)\n",
    "    piece_diff = calculate_piece_differential(board)\n",
    "    mobility = calculate_mobility(board)\n",
    "    king_safety = calculate_king_safety(board)\n",
    "    control_key = calculate_control_of_key_squares(board)\n",
    "    pawn_struct = calculate_pawn_structure(board)\n",
    "    doubled_pawns_diff = pawn_struct['doubled_pawns_diff']\n",
    "    isolated_pawns_diff = pawn_struct['isolated_pawns_diff']\n",
    "    \n",
    "    # Combine\n",
    "    extra_features = np.array([piece_diff, mobility, king_safety, control_key, doubled_pawns_diff, isolated_pawns_diff], dtype=np.float32)\n",
    "    \n",
    "    # Reshape for prediction (add batch dimension)\n",
    "    board_input = np.expand_dims(board_tensor, axis=0)   # Shape: (1, 8, 8, 13)\n",
    "    extra_input = np.expand_dims(extra_features, axis=0) # Shape: (1, 6)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict([board_input, extra_input])\n",
    "    return prediction[0, 0]\n",
    "\n",
    "model_path = \"final_model/best_model.h5\"\n",
    "\n",
    "fen = \"r2q1rk1/ppp2ppp/2np1n2/3N4/2P1PB2/3P4/PP3PPP/R2Q1RK1 w - - 0 15\"\n",
    "\n",
    "print(f\"Input state: {fen}\")\n",
    "pred = predict_from_fen(fen, model_path = model_path)\n",
    "print()\n",
    "print(\"Predicted probability of White win:\", pred)\n",
    "print()\n",
    "board = chess.Board(fen)\n",
    "chess.svg.board(board, size=350)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Tensorflow 2.13.0",
   "language": "python",
   "name": "tensorflow-2.13.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
